{ "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir(\"~/wkdir\") # change to the wkdir\n",
    "#---\n",
    "from collections import OrderedDict\n",
    "import sys\n",
    "sys.path.insert(0, \"path/to/trackcluster\") # add trackcluster to pythonpath"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Assume the path of trackcluster is path/to/trackcluster\n",
    "- Change the path/to/trackcluster to real $TrackclusterPath of trackcluster location in your own run\n",
    "\n",
    "\n",
    "#### files needed\n",
    "1. The reference file: GCF_000001735.4_TAIR10.1_genomic.fna all.fa\n",
    "2. the nanopore read file: nanopore_reads.fa\n",
    "3. reference annotation file: tair.gff\n",
    "4. Optional:the faSize output of reads from each groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import trackcluster\n",
    "from trackcluster.tracklist import read_bigg, write_bigg, bigglist_to_bedfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "# the mapping using minimap2 using all the reads (4 libs)\n",
    "minimap2 -ax splice -k14 -uf -t 10 \\\n",
    "    ~/reference/tair/GCF_000001735.4_TAIR10.1_genomic.fna nanopore_reads.fa > aln.sam\n",
    "\n",
    "samtools view -bS aln.sam>aln.bam\n",
    "samtools sort aln.bam >aln_s.bam\n",
    "samtools index aln_s.bam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "# convert the bam mapping to bigg format \n",
    "$TrackclusterPath/trackcluster/script/bam2bigg.py -b aln_s.bam -o nano.bed\n",
    "bedtools sort -i nano.bed > nanos.bed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "#convert the reference gff file to the track file in bigGenPred format, the bed file can be visualized in IGV or UCSC\n",
    "$TrackclusterPath/trackcluster/script/gff2bigg.py -k Gene -i tair.gff -o tair11.bed\n",
    "bedtools sort -i tair11.bed > tair11s.bed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "# run a quick bedtool check to assign gene name to each reads\n",
    "bedtools intersect -wa -wb -r  -f 0.05 -a nanos.bed -b tair11s.bed > nano_inter_ref.bed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional, get the dic of read:group, if any\n",
    "read_n={}\n",
    "\n",
    "f=open(\"group1.fa.sizes\")\n",
    "for line in f.readlines():\n",
    "    line_l=line.split(\"\\t\")\n",
    "    name=line_l[0]\n",
    "    group=\"group1\"\n",
    "    read_n[name] = group\n",
    "f.close()\n",
    "\n",
    "f=open(\"group2.fa.sizes\")\n",
    "for line in f.readlines():\n",
    "    line_l=line.split(\"\\t\")\n",
    "    name=line_l[0]\n",
    "    group=\"group12\"\n",
    "    read_n[name] = group\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the dic of read:gene\n",
    "read_gene={}\n",
    "f=open(\"/home/zhaolab1/data/outer/shoudong/nano_inter_ref.bed\")\n",
    "for line in f.readlines():\n",
    "    line_l=line.split(\"\\t\")\n",
    "    name=line_l[3]\n",
    "    gene=line_l[-3]\n",
    "    read_gene[name] = gene\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nano_bigg=read_bigg(\"nanos.bed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add the infor from read_gene, read_n to the track file\n",
    "nano_bigg_new=[]\n",
    "for bigg in nano_bigg:\n",
    "    try:\n",
    "        gene=read_gene[bigg.name]\n",
    "        group=read_n[bigg.name] # Optional\n",
    "\n",
    "        bigg.geneName=gene\n",
    "        bigg.geneName2=group # Optional\n",
    "        \n",
    "        nano_bigg_new.append(bigg)\n",
    "    except KeyError:\n",
    "        pass "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def group_bigg_by_gene(bigglist):\n",
    "    gene_bigg=OrderedDict()\n",
    "    \n",
    "    for bigg in bigglist:\n",
    "        try:\n",
    "            gene_bigg[bigg.geneName].append(bigg)\n",
    "        except KeyError:\n",
    "            gene_bigg[bigg.geneName]=[]\n",
    "            gene_bigg[bigg.geneName].append(bigg)\n",
    "    return gene_bigg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gene_nano=group_bigg_by_gene(nano_bigg_new)\n",
    "# for annotation\n",
    "anno_bigg=read_bigg(\"tair11s.bed\")\n",
    "gene_anno=group_bigg_by_gene(anno_bigg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write the new file with group information to make a backup\n",
    "write_bigg(nano_bigg_new, \"nanogroup.bed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from trackcluster.batch import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run the clustering for each gene \n",
    "subk=gene_nano.keys()\n",
    "len(subk)\n",
    "errors_ll=[]\n",
    "\n",
    "def process_one_subsample_try(key, intronweight=0.5, by=\"ratio_all\", full=False):\n",
    "    print key\n",
    "    gff_file = \"./\" + key + \"/\" + key + \"_gff.bed\"\n",
    "    nano_file = \"./\" + key + \"/\" + key + \"_nano.bed\"\n",
    "    figout = \"./\" + key + \"/\" + key + \"_coverage.pdf\"\n",
    "    biggout = \"./\" + key + \"/\" + key + \"_simple_coverage2.bed\"\n",
    "    Dout = \"./\" + key + \"/\" + key + \"_simple_coverage2.csv\"\n",
    "\n",
    "    if full is False:\n",
    "        if os.stat(nano_file).st_size == 0:  # no bigg nano file\n",
    "            return 0\n",
    "        if os.path.isfile(biggout):  # already processed\n",
    "            return 0\n",
    "\n",
    "    bigg_gff = read_bigg(gff_file)\n",
    "    bigg_nano_raw = read_bigg(nano_file)\n",
    "\n",
    "    try:\n",
    "        bigg_nano = prefilter_smallexon(bigg_nano_raw, bigg_gff, cutoff=50)\n",
    "        n_count = 100\n",
    "        n = 0\n",
    "\n",
    "        if bigg_nano is None:\n",
    "            return 0\n",
    "\n",
    "        try:\n",
    "            while n < n_count and len(bigg_nano) > batchsize:\n",
    "                # print \"n=\", n\n",
    "                bigg_1 = bigg_nano[:batchsize]\n",
    "                bigg_2 = bigg_nano[batchsize:]\n",
    "                _, bigg_list_by1 = flow_cluster(bigg_1, bigg_gff, by, intronweight=intronweight)\n",
    "                bigg_nano = add_subread_bigg(bigg_list_by1 + bigg_2)\n",
    "                n += 1\n",
    "                \n",
    "            D, bigg_nano_new = flow_cluster(bigg_nano, bigg_gff, by, intronweight=intronweight)\n",
    "            bigg_nano_new = add_subread_bigg(bigg_nano_new)\n",
    "\n",
    "            ### save nessary files\n",
    "            for bigg in bigg_nano_new:\n",
    "                bigg.write_subread()\n",
    "\n",
    "            bigg_count_write(bigg_nano_new, out=biggout)\n",
    "        except Exception as e:\n",
    "            errors_ll.append((key,e))\n",
    "    except Exception as e:\n",
    "        errors_ll.append((key,e))\n",
    "\n",
    "    return 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from trackcluster.utils import parmap\n",
    "# run the clustring using 40 cores\n",
    "parmap(process_one_subsample_try, subk, 40)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
